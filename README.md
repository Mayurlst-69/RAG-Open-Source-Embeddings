<div align="center">

# ðŸ“š Serverless RAG & Open Source Embeddings

![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![LangChain](https://img.shields.io/badge/LangChain-Integration-1C3C3C?style=for-the-badge&logo=langchain)
![API](https://img.shields.io/badge/LLM-Llama3_(Groq)-orange?style=for-the-badge)
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Mayurlst-69/RAG-Open-Source-Embeddings/blob/main/Code.ipynb)

<br>

**A high-performance RAG system running entirely on Free Tier resources.**
<br>
*Fast Inference â€¢ Open Source Embeddings â€¢ Zero Cost*

</div>

---

## ðŸŒŸ Overview
This project demonstrates how to build a Retrieval-Augmented Generation (RAG) pipeline using **Groq API** for ultra-fast inference and **HuggingFace Embeddings** for high-quality retrieval, all within Google Colab.

## ðŸ›  Tech Stack
| Component | Technology | Description |
| :--- | :--- | :--- |
| **LLM** | llama-3.3-70b-versatile | Via Groq API (Free Beta) |
| **Embeddings** | BAAI/bge-m3 | High MTEB Score Model |
| **Vector DB** | ChromaDB | Lightweight & Local |
| **Framework** | LangChain | Orchestration |

## ðŸš€ Quick Start

### 1. Clone the repo
```bash
git clone [https://github.com/Mayurlst-69/RAG-Open-Source-Embeddings.git](https://github.com/Mayurlst-69/RAG-Open-Source-Embeddings.git)
```
### 2. Install requirements
```bash
pip install -r requirements.txt
```
### 3. Run
```bash
Open RAG_Project.ipynb and follow the steps!
```
<div align="center">
Create by [Mayurlst-69]
</div>
