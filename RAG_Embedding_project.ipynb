{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-community pypdf sentence-transformers chromadb openai langchain-text-splitters"
      ],
      "metadata": {
        "id": "1nK5mAgBlgX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maDtFzAtg8WS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# 1. Setup API Key\n",
        "os.environ[\"API_KEY\"] = userdata.get(\"API_KEY\")\n",
        "\n",
        "# Imports for RAG Pipeline\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose your embedding Model\n",
        "EMBEDDING_MODEL_NAME = \"BAAI/bge-m3\" # Ihis model is for Multilingual\n",
        "\n",
        "print(f\"Loading Embedding Model: {EMBEDDING_MODEL_NAME}...\")\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name = EMBEDDING_MODEL_NAME,\n",
        "    model_kwargs = {'device': 'cpu'}, # Use cpu in colab\n",
        "    encode_kwargs = {'normalize_embeddings': True} # For metric, Dot Product / Cosine Sim\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "n0tBobYpqHmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load PDF\n",
        "pdf_path = \"your.pdf\" # Change your flie name\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "docs = loader.load()\n",
        "\n",
        "# 2. Split Text (Chunking)\n",
        "# Chunk Size around: 500-1000\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "print(f\"Split into {len(splits)} chunks.\")\n",
        "\n",
        "# 3. Store in Vector Database (ChromaDB)\n",
        "# Store in temp Memory or persist_directory\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=\"./chroma_db\"\n",
        ")\n",
        "\n",
        "print(\"âœ… Vector Database Ready!\")"
      ],
      "metadata": {
        "id": "hXG2rHFtr6hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive #This is for making Colab read your database from Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "y1SvUwn-aK0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup for generation\n",
        "client = OpenAI(\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=os.environ[\"API_KEY\"]\n",
        ")\n",
        "\n",
        "def query_rag(question):\n",
        "    # Step 1: Retrieval\n",
        "    # k=5 top 5 nearest\n",
        "    results = vectorstore.similarity_search(question, k=5)\n",
        "\n",
        "    # Combine result for Context\n",
        "    context_text = \"\\n\\n\".join([doc.page_content for doc in results])\n",
        "\n",
        "    # Step 2: Prompt Engineering\n",
        "    system_prompt = \"\"\"\n",
        "    You are a helpful assistant. Use the provided context to answer the user's question.\n",
        "    If the answer is not in the context, say you don't know.\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "    Context:\n",
        "    {context_text}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 3: Generation (Send to LLM)\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        model=\"llama-3.3-70b-versatile\",\n",
        "        temperature=0, # adjust for fancy output\n",
        "    )\n",
        "\n",
        "    return chat_completion.choices[0].message.content\n",
        "\n",
        "# --- Run ---\n",
        "my_question = \"prompt\"\n",
        "answer = query_rag(my_question)\n",
        "\n",
        "print(f\"ðŸ¤– Answer:\\n{answer}\")"
      ],
      "metadata": {
        "id": "Yzl0Ayx5yWtF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}